{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "A widely used technique in NLP (natural language processing). It's a great approach to start with for any text-based problem. It's also the basis of many other more advanced methods. \n",
    "\n",
    "### Tokenization and transformation\n",
    "\n",
    "The splitting of text into pieces is known as tokenization. The most common way to split is on words, but in some cases (for example in character based langauges) you may want to split on character or split on pairs or groups of words or even something more advanced. \n",
    "\n",
    "Groups of words in a split are known as n-gram. Two or three word combinations are known as bigrams and trigrams. Bigram exmaple: 'the lazy', 'brown fox' and trigrams 'brown fox jumps', 'jumps over the'\n",
    "\n",
    "#### transformation\n",
    "\n",
    "such as reducing all letters to lower case to prevent fox and Fox counting as 2 seperate accounts. \n",
    "\n",
    "#### Stemming\n",
    "\n",
    "which strips word suffices can also be a transformation technique for extracting more signals out of different words with simiilar meanings. i.e. jump, jumping, jumps, jumped to al be expressed as jump\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "After defining the dictionary you can convert any text to a set of numbers corresponding to the occurences of each dictionary word in the text. \n",
    "\n",
    "##### Stop Words\n",
    "\n",
    "words that are generally not that important or meaningless i.e. 'the', 'is', 'and'. Most ML engineers will remove the stop words and most libraries have a pre-stop word list.\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One problem with bag of words models is the nature of simple word counts. if a non-stop-word is common in the corups for example 'data'. It's not necessarily infomrative to konw that the word also appears in a new text. Instead, you'd do better by focusing on relatively rare words that are more highly predictive of the outcome of interest. \n",
    "\n",
    "To this end, it's common to scale the word counts by the inverse of the total count of that word in the corpus. Because we're describing a corpus in numbers. If there is an abundent count of a word in the training corpus but not in the new document then there is some meaning there. This means preferring rare words over common to find meaning in the differences in the rare ones. \n",
    "\n",
    "#### term frequency-inverse document frequency (tf-idf)\n",
    "\n",
    "This algo is commonly used to handle this issue. It calculates a product of the term frequency and inverses the document frequency. \n",
    "\n",
    "#### Laten semantic analysis (lsa) or latent semantic indexing (lsi)\n",
    "\n",
    "The ideas is to use the bag of word counts to build a term document matrix, with a row for each term and a column for each document. The elements of this matrix are then normalized similarly to the tf-idf process in order to avoid frequent terms dominating the power of the matrix. \n",
    "\n",
    "The value of this is there are themes or concepts that the LSA can pattern out. For example 'dog' may have related words such as 'barking', 'kennel' so on. \n",
    "\n",
    "##### singular value decomposition (SVD)\n",
    "\n",
    "you split the term document into 3 matrices (T,S,D). T is the term-concept matrix that relates the term (barking or kennel) to concepts (dog) and D is the concept document matrix that relates individual documents to concepts that you'll later use to extract the features from the LSA model. \n",
    "\n",
    "The S matrix holds the singular values. These denote the relative importance that a term has to a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_semantic_analysis(docs):\n",
    "    tfidf = TfidfVectorizer() #this uses default params\n",
    "    tfidf.fit(docs) #creates the dictionary\n",
    "    vecs = tfidf.transform(docs) #uses dictionary to vectorize documents\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    svd.fit(vecs) #creating SVD matrices\n",
    "    return svd.transform(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24972705, -0.06943167, -0.01310536, ..., -0.01598168,\n",
       "        -0.01356266, -0.03298139],\n",
       "       [ 0.1399918 , -0.07671413, -0.03974792, ..., -0.00946167,\n",
       "        -0.01077104,  0.00186035],\n",
       "       [ 0.37184255, -0.04142757, -0.06709439, ...,  0.01893997,\n",
       "         0.03521554,  0.00430493],\n",
       "       ..., \n",
       "       [ 0.18476811, -0.00611307, -0.08039222, ...,  0.02708606,\n",
       "         0.00208008,  0.00291649],\n",
       "       [ 0.18795807, -0.06606594,  0.0415711 , ..., -0.01028309,\n",
       "        -0.0177876 ,  0.01607466],\n",
       "       [ 0.08231697, -0.09080525,  0.00372935, ..., -0.01757763,\n",
       "        -0.04046399,  0.01610777]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "latent_semantic_analysis(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### probailistic method (pLSA) or latent Dirichlet Analysis (LDA)\n",
    "\n",
    "LSA is based on linear algebra (math with vectors and matrices) but an equivalent analysis scan be done using probabilistic methods that model each document as a statistical mixture of topic distrubitions. \n",
    "\n",
    "The specific assumptions are made on the distribution of topics. You build an the assumption that a document can be described by a small set of topics and that ay term (word) can be attributed to a topic. In practice, LDA, can perform well on diverse datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skip\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_model(docs):\n",
    "    #build lda model and set the number of topics to extract\n",
    "    return LdaModel(docs, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_vector(lda_model, doc):\n",
    "    #gen features for a new documents\n",
    "    return lda_model[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import mock_data\n",
    "gensim_corpus = mock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = lda_model(gensim_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Expansion\n",
    "\n",
    "#### follow links\n",
    "\n",
    "A good example of link following is establish twitter sentiment. A blob of short characters may not be enough to understand sentiment so you may follow any links that are posted with the tweet as well and expand the tweet with the text from thelink to get a better idea. \n",
    "\n",
    "#### knowledge-base expansion\n",
    "\n",
    "You could identify named entitites in the text and extend the original text with information about each named entity in an online knowledge base, think wikipedia. As you hit named entitites you would go out and grab text from a wikipedia entry and perform any of the text extraction algos on said entry.\n",
    "\n",
    "This is a non-trivial task and there are plenty of research articls out on it. \n",
    "\n",
    "#### Text meta-features\n",
    "\n",
    "This is a problem-dependent method. A tweet contains all sorts of valuable data that's particular to tweets and can be extracted for example hashtags, mentions and meta-info from twitter such as counts of retweets and favorites. \n",
    "\n",
    "For web based text, you could extract basic info form the link text such as the top level domain. \n",
    "\n",
    "## Image Features\n",
    "\n",
    "### Simple image features\n",
    "\n",
    "You make a single row with all pixels, converting the two dimensional image into one. If a color you have 3 images in one (red, blue, green channels). Normal pixel values are 0 to 1 or 0 to 255. Because of the size of modern images it can be said this process could easilly lead to overfitting\n",
    "\n",
    "### Color features\n",
    "\n",
    "If you are trying to categorize landscape images (sky, mountain, grass) it may be useful to represent by constituted colors. \n",
    "\n",
    "simple color statistics of each color channel of the image such as mean, median, mode, std deviation, skewness, kurtosis. This makes 18 features assuming a colored picture in RGB scale. 6 x 3. \n",
    "\n",
    "### Image metadata features\n",
    "\n",
    "Some meta data examples\n",
    "\n",
    "* manufacturer orientation landscape, portrait\n",
    "* date-time\n",
    "* compression jpeg, raw\n",
    "* resolution\n",
    "* aspect ratio\n",
    "* exposure time\n",
    "* aperture\n",
    "* flash \n",
    "* focal length\n",
    "\n",
    "## Extracting objects and shapes\n",
    "\n",
    "### edge detection\n",
    "\n",
    "the simplest way to represent shapes in images is to find their edges and build features on those. \n",
    "\n",
    "A few common algos\n",
    "\n",
    "* sobel\n",
    "* canny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7ec49fdb3e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcamera\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msobel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import skimage\n",
    "\n",
    "image = skimage.data.camera()\n",
    "edges = skimage.filter.sobel(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced-shape features\n",
    "\n",
    "#### Histogram of oriented Gradients (hog)\n",
    "\n",
    "1. calc the gradient image (which direction the edges of the image are 'moving')\n",
    "2. divide the image into small blocks called cells\n",
    "3. calculate the orientation of the gradients inside those cells\n",
    "4. calculate the histogram of those orientations in the individual cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import data, color, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = color.rgb2gray(skimage.data.astronaut())\n",
    "feature.hog(image, orientations=9, pixels_per_cell=(8,8), cells_per_block=(3,3), visualise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "When performing feature extraction- dimensionality reduction should be top of mind. \n",
    "\n",
    "#### Principle Component Analysis (PCA)\n",
    "Allows you to takea  set of images and find 'typical' images that can be used as building blocks to represent the original images. Combing the first couple of principle components enables you to rebuild a large portion of the training images, whereas subsequent components will cover less-frequent patterns. \n",
    "\n",
    "These are exclusively linear algorithms. As features are generated by finding the 'distance' from a principle image.\n",
    "\n",
    "#### Automatic feature extraction\n",
    "\n",
    "Think of deep neural nets as the best way to automatically extract features. \n",
    "\n",
    "#### Time-series features\n",
    "\n",
    "##### Classical time series\n",
    "\n",
    "Is the numerical measurements that are taken over time. These are evenly spaced over time (hourly, monthly) but can have irregular data. \n",
    "\n",
    "1. value of the stock market measured hourly\n",
    "2. Day to day energy consumption of a commercial building or residential home\n",
    "3. the value, in dollars, of a client's bank account over time\n",
    "4. sets of diagnostics monitored in an industrial manufacturing plant\n",
    "\n",
    "##### Point processes\n",
    "Is a collection of events that occur over time. As opposed to measuring numerical quantities over time, these re timestamp for each discrete event that happens plus the potential of other metadata bout the event. They are commonly called event streams. \n",
    "\n",
    "1. activity of a web users, measuring the time and type of each click (clickstream data)\n",
    "2. worldwide occurences of earthquakes, hurricanes, disease outbreak and so forth\n",
    "3. individual purchases made by a customer through the history of their account\n",
    "4. event logs in a manufacturing plant, recording every time an employee touches teh system and every time a step in the manufacturing process is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidntNum</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>PdDistrict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30203898</td>\n",
       "      <td>FRAUD</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>16:30</td>\n",
       "      <td>NORTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38261</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>04/17/2003</td>\n",
       "      <td>22:45</td>\n",
       "      <td>NORTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30203901</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>16:05</td>\n",
       "      <td>NORTHERN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30203923</td>\n",
       "      <td>DRUG/NARCOTIC</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>17:00</td>\n",
       "      <td>BAYVIEW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203923</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>17:00</td>\n",
       "      <td>BAYVIEW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IncidntNum        Category        Date   Time PdDistrict\n",
       "0    30203898           FRAUD  02/18/2003  16:30   NORTHERN\n",
       "1       38261        WARRANTS  04/17/2003  22:45   NORTHERN\n",
       "2    30203901   LARCENY/THEFT  02/18/2003  16:05   NORTHERN\n",
       "3    30203923   DRUG/NARCOTIC  02/18/2003  17:00    BAYVIEW\n",
       "4    30203923  OTHER OFFENSES  02/18/2003  17:00    BAYVIEW"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/sfpd_incident_all.csv\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Month'] = map(lambda x: datetime.strptime(\"/\".join(x.split(\"/\")[0:2]), \"%m/%y\"), df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IncidntNum</th>\n",
       "      <th>Category</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30203898</td>\n",
       "      <td>FRAUD</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>16:30</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38261</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>04/17/2003</td>\n",
       "      <td>22:45</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2017-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30203901</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>16:05</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30203923</td>\n",
       "      <td>DRUG/NARCOTIC</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>17:00</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30203923</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>02/18/2003</td>\n",
       "      <td>17:00</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IncidntNum        Category        Date   Time PdDistrict      Month\n",
       "0    30203898           FRAUD  02/18/2003  16:30   NORTHERN 2018-02-01\n",
       "1       38261        WARRANTS  04/17/2003  22:45   NORTHERN 2017-04-01\n",
       "2    30203901   LARCENY/THEFT  02/18/2003  16:05   NORTHERN 2018-02-01\n",
       "3    30203923   DRUG/NARCOTIC  02/18/2003  17:00    BAYVIEW 2018-02-01\n",
       "4    30203923  OTHER OFFENSES  02/18/2003  17:00    BAYVIEW 2018-02-01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ts = df.groupby('Month').aggregate(len)['IncidntNum']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a968b7473a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#plot time series\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_ts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-k'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Month'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of Crimes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "#plot time series\n",
    "plot(df_ts.index, df_ts.values, '-k', lw=2)\n",
    "xlabel('Month')\n",
    "ylabel('Number of Crimes')\n",
    "ylim((8000, 14000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Simple time-series features\n",
    "\n",
    "average the mean of the measurements can uncover tendencies in the average value of a time series\n",
    "spread measurements of the spread of a distribution, such as a standard deviation, median absolute deviation, or interquartile range, can reveal trends in the overall variability of the measurements\n",
    "outliers the frequencey of time-series measurements tha tfall outside of the range of typical distro\n",
    "* Distribution: estimating the higher order characteristics of the marginal distribution of the time-series measurements\n",
    "\n",
    "#### Windowed statistics\n",
    "\n",
    "entalis calculating the preceding summary metrics within a specified time window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window1 = (datetime(2015, 3, 22), datetime(2015,6,21))\n",
    "\n",
    "#find which data points fall within the window\n",
    "idx_window = np.where(map(lambda x: x>=window1[0] and x<=window1[1], df_ts.index))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windowed mean and std deviatoin\n",
    "mean_window = np.mean(df_ts.values[idx_window])\n",
    "std_window = np.std(df_ts.values[idx_window])\n",
    "\n",
    "#windowed differences:\n",
    "# window 2 = sprint 2013\n",
    "window2 = (datetime(2013,3,22), datetime(2013,6,21))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fine which date points fall within the window\n",
    "idx_window2 = np.where(map(lambda x: x>=window2[0] and x<=window2[1], df_ts.index))[0]\n",
    "\n",
    "#windowed differences: mean and std deviation\n",
    "mean_wdiff = mean_window - np.mean(df_ts.values[idx_window2])\n",
    "std_wdiff = std_window - np.std(df_ts.values[idx_window2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrection\n",
    "\n",
    "measures the statistical correlation of a time series with a lagged version of itself. for example, the one autocorrelation feature of a time series takes the original time sereis and correlates it with the same time series shifted over by one time bin to the left. By shifting the time series like this, you can capture the presence of periodicity and other statistical structure in the time series. \n",
    "\n",
    "The shape of the auocorr function caputres the esences ofr the structure of the time series. The statsmodels contains an easy to use autocorr function \n",
    "\n",
    "### Forurier analysis\n",
    "\n",
    "A common time series feature engineering tool. The gaol of the fourier analyssis is to decompose a time series into a sum of sine and cosine functions on a range of frequences, which are naturally occuring in manyu real-world datasets.\n",
    "\n",
    "#### periodogram\n",
    "is the decomposition of a time series into its component spectral densisities. \n",
    "\n",
    "#### short list of time series models\n",
    "\n",
    "* autoregressive (AR) model:\n",
    "each value in the time series is modeled as a linear combination of the last p vlaues, where p is a free parameter to be stimated\n",
    "\n",
    "* autoregressive-moving average (arma):\n",
    "each value is modeled as the sum of two polynomial functions: the ar model and a moving-average model that's a linear combination of the previous q error terms\n",
    "* GARCH model:\n",
    "a model commonly used in financial analysis that describes the random noise terms of a time series using an ARMA model\n",
    "* Hidden Markov model (HMM):\n",
    "a probabilistic model that describes the observed values of the time series as a being drawn from a series of hidden states which themslves follow a markov process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodogram feature example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "f, psd = scipy.signal.periodogram(df_ts, detrend='linear')\n",
    "plt.plot(f, psd, '-ob')\n",
    "plt.xlabl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
