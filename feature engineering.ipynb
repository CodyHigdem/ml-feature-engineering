{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    "## Bag of words\n",
    "\n",
    "A widely used technique in NLP (natural language processing). It's a great approach to start with for any text-based problem. It's also the basis of many other more advanced methods. \n",
    "\n",
    "### Tokenization and transformation\n",
    "\n",
    "The splitting of text into pieces is known as tokenization. The most common way to split is on words, but in some cases (for example in character based langauges) you may want to split on character or split on pairs or groups of words or even something more advanced. \n",
    "\n",
    "Groups of words in a split are known as n-gram. Two or three word combinations are known as bigrams and trigrams. Bigram exmaple: 'the lazy', 'brown fox' and trigrams 'brown fox jumps', 'jumps over the'\n",
    "\n",
    "#### transformation\n",
    "\n",
    "such as reducing all letters to lower case to prevent fox and Fox counting as 2 seperate accounts. \n",
    "\n",
    "#### Stemming\n",
    "\n",
    "which strips word suffices can also be a transformation technique for extracting more signals out of different words with simiilar meanings. i.e. jump, jumping, jumps, jumped to al be expressed as jump\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "After defining the dictionary you can convert any text to a set of numbers corresponding to the occurences of each dictionary word in the text. \n",
    "\n",
    "##### Stop Words\n",
    "\n",
    "words that are generally not that important or meaningless i.e. 'the', 'is', 'and'. Most ML engineers will remove the stop words and most libraries have a pre-stop word list.\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One problem with bag of words models is the nature of simple word counts. if a non-stop-word is common in the corups for example 'data'. It's not necessarily infomrative to konw that the word also appears in a new text. Instead, you'd do better by focusing on relatively rare words that are more highly predictive of the outcome of interest. \n",
    "\n",
    "To this end, it's common to scale the word counts by the inverse of the total count of that word in the corpus. Because we're describing a corpus in numbers. If there is an abundent count of a word in the training corpus but not in the new document then there is some meaning there. This means preferring rare words over common to find meaning in the differences in the rare ones. \n",
    "\n",
    "#### term frequency-inverse document frequency (tf-idf)\n",
    "\n",
    "This algo is commonly used to handle this issue. It calculates a product of the term frequency and inverses the document frequency. \n",
    "\n",
    "#### Laten semantic analysis (lsa) or latent semantic indexing (lsi)\n",
    "\n",
    "The ideas is to use the bag of word counts to build a term document matrix, with a row for each term and a column for each document. The elements of this matrix are then normalized similarly to the tf-idf process in order to avoid frequent terms dominating the power of the matrix. \n",
    "\n",
    "The value of this is there are themes or concepts that the LSA can pattern out. For example 'dog' may have related words such as 'barking', 'kennel' so on. \n",
    "\n",
    "##### singular value decomposition (SVD)\n",
    "\n",
    "you split the term document into 3 matrices (T,S,D). T is the term-concept matrix that relates the term (barking or kennel) to concepts (dog) and D is the concept document matrix that relates individual documents to concepts that you'll later use to extract the features from the LSA model. \n",
    "\n",
    "The S matrix holds the singular values. These denote the relative importance that a term has to a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def latent_semantic_analysis(docs):\n",
    "    tfidf = TfidfVectorizer() #this uses default params\n",
    "    tfidf.fit(docs) #creates the dictionary\n",
    "    vecs = tfidf.transform(docs) #uses dictionary to vectorize documents\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    svd.fit(vecs) #creating SVD matrices\n",
    "    return svd.transform(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24972705, -0.06943154, -0.01310705, ...,  0.05060385,\n",
       "         0.02069774,  0.01851646],\n",
       "       [ 0.1399918 , -0.07671322, -0.03975507, ...,  0.06868958,\n",
       "         0.00248835,  0.0073129 ],\n",
       "       [ 0.37184255, -0.04142798, -0.0670953 , ..., -0.03153767,\n",
       "        -0.00784163, -0.02100757],\n",
       "       ..., \n",
       "       [ 0.18476811, -0.00611318, -0.08038995, ..., -0.02209117,\n",
       "        -0.00948749, -0.02047383],\n",
       "       [ 0.18795807, -0.06606492,  0.04157621, ...,  0.0360893 ,\n",
       "        -0.00985462, -0.00469434],\n",
       "       [ 0.08231697, -0.09080726,  0.00372898, ...,  0.00083822,\n",
       "         0.01503654,  0.01007338]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "latent_semantic_analysis(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### probailistic method (pLSA) or latent Dirichlet Analysis (LDA)\n",
    "\n",
    "LSA is based on linear algebra (math with vectors and matrices) but an equivalent analysis scan be done using probabilistic methods that model each document as a statistical mixture of topic distrubitions. \n",
    "\n",
    "The specific assumptions are made on the distribution of topics. You build an the assumption that a document can be described by a small set of topics and that ay term (word) can be attributed to a topic. In practice, LDA, can perform well on diverse datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skip\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_model(docs):\n",
    "    #build lda model and set the number of topics to extract\n",
    "    return LdaModel(docs, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_vector(lda_model, doc):\n",
    "    #gen features for a new documents\n",
    "    return lda_model[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import mock_data\n",
    "gensim_corpus = mock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = lda_model(gensim_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Expansion\n",
    "\n",
    "#### follow links\n",
    "\n",
    "A good example of link following is establish twitter sentiment. A blob of short characters may not be enough to understand sentiment so you may follow any links that are posted with the tweet as well and expand the tweet with the text from thelink to get a better idea. \n",
    "\n",
    "#### knowledge-base expansion\n",
    "\n",
    "You could identify named entitites in the text and extend the original text with information about each named entity in an online knowledge base, think wikipedia. As you hit named entitites you would go out and grab text from a wikipedia entry and perform any of the text extraction algos on said entry.\n",
    "\n",
    "This is a non-trivial task and there are plenty of research articls out on it. \n",
    "\n",
    "#### Text meta-features\n",
    "\n",
    "This is a problem-dependent method. A tweet contains all sorts of valuable data that's particular to tweets and can be extracted for example hashtags, mentions and meta-info from twitter such as counts of retweets and favorites. \n",
    "\n",
    "For web based text, you could extract basic info form the link text such as the top level domain. \n",
    "\n",
    "## Image Features\n",
    "\n",
    "### Simple image features\n",
    "\n",
    "You make a single row with all pixels, converting the two dimensional image into one. If a color you have 3 images in one (red, blue, green channels). Normal pixel values are 0 to 1 or 0 to 255. Because of the size of modern images it can be said this process could easilly lead to overfitting\n",
    "\n",
    "### Color features\n",
    "\n",
    "If you are trying to categorize landscape images (sky, mountain, grass) it may be useful to represent by constituted colors. \n",
    "\n",
    "simple color statistics of each color channel of the image such as mean, median, mode, std deviation, skewness, kurtosis. This makes 18 features assuming a colored picture in RGB scale. 6 x 3. \n",
    "\n",
    "### Image metadata features\n",
    "\n",
    "Some meta data examples\n",
    "\n",
    "* manufacturer orientation landscape, portrait\n",
    "* date-time\n",
    "* compression jpeg, raw\n",
    "* resolution\n",
    "* aspect ratio\n",
    "* exposure time\n",
    "* aperture\n",
    "* flash \n",
    "* focal length\n",
    "\n",
    "## Extracting objects and shapes\n",
    "\n",
    "### edge detection\n",
    "\n",
    "the simplest way to represent shapes in images is to find their edges and build features on those. \n",
    "\n",
    "A few common algos\n",
    "\n",
    "* sobel\n",
    "* canny\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name filter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ed28c123b452>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcamera\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0medges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msobel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name filter"
     ]
    }
   ],
   "source": [
    "from skimage import data, filter\n",
    "image = skimage.data.camera()\n",
    "edges = skimage.filter.sobel(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced-shape features\n",
    "\n",
    "#### Histogram of oriented Gradients (hog)\n",
    "\n",
    "1. calc the gradient image (which direction the edges of the image are 'moving')\n",
    "2. divide the image into small blocks called cells\n",
    "3. calculate the orientation of the gradients inside those cells\n",
    "4. calculate the histogram of those orientations in the individual cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage import data, color, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01901523,  0.01426503,  0.        , ...,  0.01419268,\n",
       "         0.01640198,  0.00489649]),\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.01025812, ...,  0.00111188,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.00508466,  0.        , ...,  0.        ,\n",
       "          0.00064617,  0.        ],\n",
       "        ..., \n",
       "        [ 0.        ,  0.00091809,  0.        , ...,  0.        ,\n",
       "          0.01232994,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.01886175, ...,  0.01850313,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = color.rgb2gray(skimage.data.astronaut())\n",
    "feature.hog(image, orientations=9, pixels_per_cell=(8,8), cells_per_block=(3,3), visualise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
